#!/usr/bin/env python3
import os
import requests
import time
import json
import threading
from concurrent.futures import ThreadPoolExecutor
from colorama import init, Fore, Back, Style
from urllib.parse import urlparse

# Inicializar colorama
init(autoreset=True)

# Configura√ß√µes
PASTA_RESULTADOS = "ErikNet_Results"
os.makedirs(PASTA_RESULTADOS, exist_ok=True)
MAX_THREADS = 10  # N√∫mero m√°ximo de threads para buscas paralelas

# Banner ErikNet
BANNER = Fore.CYAN + r"""
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñà‚ñà‚ñë ‚ñë‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë   ‚ñà‚ñà‚ñë     ‚ñë‚ñà‚ñà‚ñë  ‚ñë‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë ‚ñà‚ñà   ‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë
‚ñë‚ñë‚ñë‚ñà‚ñà‚ñë‚ñë ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñà‚ñà‚ñë‚ñà‚ñà  ‚ñë‚ñë      ‚ñà‚ñà‚ñë‚ñë    ‚ñë‚ñà‚ñà‚ñë  ‚ñë‚ñà‚ñà‚ñë‚ñà‚ñà   ‚ñà‚ñà ‚ñà‚ñà  ‚ñà‚ñà‚ñë ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë  ‚ñà‚ñà   ‚ñà‚ñà‚ñë
  ‚ñë‚ñà‚ñà‚ñë  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë     ‚ñà‚ñà‚ñë     ‚ñë‚ñà‚ñà‚ñë  ‚ñë‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë ‚ñà‚ñà‚ñà‚ñà‚ñë   ‚ñà‚ñà‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë
  ‚ñë‚ñà‚ñà‚ñë‚ñë ‚ñà‚ñà   ‚ñà‚ñà‚ñë‚ñà‚ñà   ‚ñë‚ñë     ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë  ‚ñë‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñë‚ñà‚ñà  ‚ñà‚ñà‚ñë ‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñë ‚ñà‚ñà‚ñë‚ñë‚ñë   ‚ñà‚ñà   ‚ñà‚ñà‚ñë‚ñë
  ‚ñë‚ñà‚ñà‚ñë‚ñë ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñà‚ñà‚ñë‚ñë‚ñë‚ñà‚ñà ‚ñà‚ñà‚ñë‚ñë ‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà   ‚ñà‚ñà‚ñë
   ‚ñë‚ñë‚ñë  ‚ñë‚ñë‚ñë ‚ñë‚ñë‚ñë ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë     ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë ‚ñë‚ñë   ‚ñë‚ñë ‚ñë‚ñë  ‚ñë‚ñë‚ñë ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë
   ‚ñë ‚ñë  ‚ñë     ‚ñë ‚ñë   ‚ñë ‚ñë     ‚ñë‚ñë   ‚ñë‚ñë  ‚ñë‚ñë‚ñë  ‚ñë‚ñë‚ñë  ‚ñë   ‚ñë  ‚ñë    ‚ñë‚ñë ‚ñë‚ñë    ‚ñë‚ñë      ‚ñë‚ñë
  ‚ñë  ‚ñë             ‚ñë   ‚ñë    ‚ñë   ‚ñë    ‚ñë  ‚ñë   ‚ñë  ‚ñë    ‚ñë       ‚ñë  ‚ñë   ‚ñë       ‚ñë 
""" + Style.RESET_ALL + Fore.YELLOW + """
  made in Brazil Big The god and Erik 16y Linux and termux 
""" + Style.RESET_ALL

def limpar_tela():
    os.system('cls' if os.name == 'nt' else 'clear')

def validar_username(username):
    """Valida se o nome de usu√°rio √© v√°lido para a maioria das plataformas"""
    if not username or len(username) < 3 or len(username) > 30:
        return False
    
    # Verifica caracteres permitidos (letras, n√∫meros, pontos, underscores)
    for char in username:
        if not (char.isalnum() or char in ['.', '_', '-']):
            return False
    
    return True

def verificar_instagram(username):
    """Verifica√ß√£o espec√≠fica para Instagram"""
    try:
        url = f"https://www.instagram.com/{username}/"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10, allow_redirects=False)
        
        if response.status_code == 200:
            return {'exists': True, 'url': url, 'method': 'Web Scraping'}
        elif response.status_code == 404:
            return {'exists': False, 'url': url, 'method': 'Web Scraping'}
        else:
            return {'error': f'Status code: {response.status_code}', 'exists': False, 'url': url, 'method': 'Web Scraping'}
    except Exception as e:
        return {'error': str(e), 'exists': False, 'url': url, 'method': 'Web Scraping'}

def verificar_github(username):
    """Verifica√ß√£o espec√≠fica para GitHub"""
    try:
        url = f"https://api.github.com/users/{username}"
        headers = {'Accept': 'application/vnd.github.v3+json'}
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            return {'exists': True, 'url': f"https://github.com/{username}", 'method': 'API', 'data': response.json()}
        elif response.status_code == 404:
            return {'exists': False, 'url': f"https://github.com/{username}", 'method': 'API'}
        else:
            return {'error': f'Status code: {response.status_code}', 'exists': False, 'url': f"https://github.com/{username}", 'method': 'API'}
    except Exception as e:
        return {'error': str(e), 'exists': False, 'url': f"https://github.com/{username}", 'method': 'API'}

def verificar_tiktok(username):
    """Verifica√ß√£o espec√≠fica para TikTok"""
    try:
        url = f"https://www.tiktok.com/@{username}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.head(url, headers=headers, timeout=10, allow_redirects=True)
        
        if response.status_code == 200 and username.lower() in response.url.lower():
            return {'exists': True, 'url': response.url, 'method': 'Web Scraping'}
        else:
            return {'exists': False, 'url': url, 'method': 'Web Scraping'}
    except Exception as e:
        return {'error': str(e), 'exists': False, 'url': url, 'method': 'Web Scraping'}

def verificar_plataforma_generica(plataforma, username, url_template, method='Web Scraping'):
    """Verifica√ß√£o gen√©rica para plataformas sem API espec√≠fica"""
    try:
        url = url_template.format(username=username)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        if method == 'API':
            response = requests.get(url, headers=headers, timeout=10)
        else:
            response = requests.head(url, headers=headers, timeout=10, allow_redirects=True)
        
        if response.status_code == 200:
            return {'exists': True, 'url': response.url if method == 'Web Scraping' else url, 'method': method}
        else:
            return {'exists': False, 'url': url, 'method': method}
    except Exception as e:
        return {'error': str(e), 'exists': False, 'url': url, 'method': method}

def buscar_perfis_paralelo(username):
    """Busca perfis em paralelo usando threads"""
    resultados = {}
    
    # Lista de redes sociais (20 brasileiras + internacionais relevantes)
    sites = {
        # Redes Brasileiras
        "Kwai": {
            "url": "https://www.kwai.com/@{username}",
            "method": "Web Scraping",
            "function": verificar_plataforma_generica
        },
        "TikTok": {
            "url": "https://www.tiktok.com/@{username}",
            "method": "Web Scraping",
            "function": verificar_tiktok
        },
        "Skoob": {
            "url": "https://www.skoob.com.br/usuario/{username}",
            "method": "Web Scraping",
            "function": verificar_plataforma_generica
        },
        "Vivver": {
            "url": "https://www.vivver.com.br/{username}",
            "method": "Web Scraping",
            "function": verificar_plataforma_generica
        },
        "Recanto": {
            "url": "https://www.recantodasletras.com.br/perfil/{username}",
            "method": "Web Scraping",
            "function": verificar_plataforma_generica
        },
        "UOL": {
            "url": "https://meu.uol.com.br/perfil/{username}",
            "method": "Web Scraping",
            "function": verificar_plataforma_generica
        },
        "Terra": {
            "url": "https://perfil.terra.com.br/{username}",
            "method": "Web Scraping",
            "function": verificar_plataforma_generica
        },
        "iFood": {
            "url": "https://www.ifood.com.br/perfil/{username}",
            "method": "Web Scraping",
            "function": verificar_plataforma_generica
        },
        "OLX": {
            "url": "https://www.olx.com.br/perfil/{username}",
            "method": "Web Scraping",
            "function": verificar_plataforma_generica
        },
        "Mercado Livre": {
            "url": "https://www.mercadolivre.com.br/perfil/{username}",
            "method": "Web Scraping",
            "function": verificar_plataforma_generica
        },
        # Redes Internacionais
        "Instagram": {
            "url": "https://www.instagram.com/{username}",
            "method": "Web Scraping",
            "function": verificar_instagram
        },
        "Facebook": {
            "url": "https://www.facebook.com/{username}",
            "method": "Web Scraping",
            "function": verificar_plataforma_generica
        },
        "Twitter": {
            "url": "https://twitter.com/{username}",
            "method": "Web Scraping",
            "function": verificar_plataforma_generica
        },
        "GitHub": {
            "url": "https://api.github.com/users/{username}",
            "method": "API",
            "function": verificar_github
        },
        "Reddit": {
            "url": "https://www.reddit.com/user/{username}",
            "method": "API",
            "function": verificar_plataforma_generica
        },
        "YouTube": {
            "url": "https://www.youtube.com/@{username}",
            "method": "Web Scraping",
            "function": verificar_plataforma_generica
        },
        "Twitch": {
            "url": "https://www.twitch.tv/{username}",
            "method": "API",
            "function": verificar_plataforma_generica
        },
        "Pinterest": {
            "url": "https://www.pinterest.com/{username}",
            "method": "Web Scraping",
            "function": verificar_plataforma_generica
        },
        "LinkedIn": {
            "url": "https://www.linkedin.com/in/{username}",
            "method": "Web Scraping",
            "function": verificar_plataforma_generica
        },
        "Telegram": {
            "url": "https://t.me/{username}",
            "method": "Web Scraping",
            "function": verificar_plataforma_generica
        }
    }

    def processar_site(site, config):
        try:
            if config['function'] == verificar_plataforma_generica:
                result = config['function'](site, username, config['url'], config['method'])
            else:
                result = config['function'](username)
            resultados[site] = result
        except Exception as e:
            resultados[site] = {
                'error': str(e),
                'exists': False,
                'url': config['url'].format(username=username),
                'method': config['method']
            }

    # Usando ThreadPoolExecutor para buscas paralelas
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        futures = []
        for site, config in sites.items():
            futures.append(executor.submit(processar_site, site, config))
        
        # Espera todas as threads terminarem
        for future in futures:
            future.result()

    return resultados

def salvar_resultados(username, dados):
    """Salva os resultados em um arquivo JSON"""
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    nome_arquivo = f"{PASTA_RESULTADOS}/{username}_{timestamp}.json"
    
    try:
        with open(nome_arquivo, 'w', encoding='utf-8') as f:
            json.dump({
                'username': username,
                'date': timestamp,
                'results': dados
            }, f, indent=4, ensure_ascii=False)
        
        return nome_arquivo
    except Exception as e:
        print(f"{Fore.RED}Erro ao salvar resultados: {str(e)}{Style.RESET_ALL}")
        return None

def mostrar_resultados_eriknet(username, dados):
    limpar_tela()
    print(BANNER)
    
    print("\n" + "‚ïê"*80)
    print(Fore.CYAN + f" RESULTADOS ERIKNET - @{username} ".center(80) + Style.RESET_ALL)
    print("‚ïê"*80)
    
    encontrados = sum(1 for info in dados.values() if info.get('exists'))
    total = len(dados)
    
    print(f"\n{Fore.YELLOW}üîç Busca conclu√≠da: {encontrados}/{total} plataformas encontradas{Style.RESET_ALL}\n")
    
    for plataforma, info in sorted(dados.items()):
        print(f"\n‚ñì {Fore.YELLOW}{plataforma.upper()}{Style.RESET_ALL}")
        
        if 'error' in info:
            print(f"  {Fore.RED}üî¥ ERRO: {info['error']}{Style.RESET_ALL}")
        else:
            if info.get('exists'):
                print(f"  {Fore.GREEN}üü¢ ENCONTRADO{Style.RESET_ALL}")
                print(f"  {Fore.BLUE}üåê URL: {info['url']}{Style.RESET_ALL}")
                
                # Op√ß√£o para copiar URL
                if input(f"  {Fore.MAGENTA}üìã Copiar URL? (s/n): {Style.RESET_ALL}").lower() == 's':
                    try:
                        import pyperclip
                        pyperclip.copy(info['url'])
                        print(f"  {Fore.GREEN}‚úÖ URL copiada para a √°rea de transfer√™ncia!{Style.RESET_ALL}")
                    except:
                        print(f"  {Fore.RED}‚ùå N√£o foi poss√≠vel copiar (pyperclip n√£o instalado){Style.RESET_ALL}")
            else:
                print(f"  {Fore.RED}üî¥ N√ÉO ENCONTRADO{Style.RESET_ALL}")
                
            print(f"  {Fore.MAGENTA}‚öôÔ∏è M√âTODO: {info['method']}{Style.RESET_ALL}")
    
    print("\n" + "‚ïê"*80)
    print(f"{Fore.YELLOW}Total de redes verificadas: {total}{Style.RESET_ALL}")
    print(f"{Fore.YELLOW}Perfis encontrados: {encontrados}{Style.RESET_ALL}")
    print("‚ïê"*80)

def menu_principal():
    limpar_tela()
    print(BANNER)
    print(f"\n{Fore.GREEN}[{time.strftime('%d/%m/%Y %H:%M:%S')}]{Style.RESET_ALL}")
    print("\n1. Buscar por nome de usu√°rio")
    print("2. Ver hist√≥rico de buscas")
    print("3. Sair")
    
    try:
        return int(input(f"\n{Fore.YELLOW}Escolha uma op√ß√£o (1-3): {Style.RESET_ALL}"))
    except:
        return 0

def ver_historico():
    limpar_tela()
    print(BANNER)
    print("\n" + "‚ïê"*60)
    print(Fore.CYAN + " HIST√ìRICO DE BUSCAS ".center(60) + Style.RESET_ALL)
    print("‚ïê"*60)
    
    try:
        arquivos = [f for f in os.listdir(PASTA_RESULTADOS) if f.endswith('.json')]
        
        if not arquivos:
            print(f"\n{Fore.RED}Nenhuma busca encontrada no hist√≥rico.{Style.RESET_ALL}")
            return
        
        print(f"\n{Fore.YELLOW}√öltimas buscas:{Style.RESET_ALL}")
        for i, arquivo in enumerate(arquivos[-10:], 1):  # Mostra os 10 √∫ltimos
            nome, data = arquivo.split('_')[:2]
            data_formatada = f"{data[6:8]}/{data[4:6]}/{data[0:4]} {data[9:11]}:{data[11:13]}"
            print(f"{i}. {nome} - {data_formatada}")
        
        print("\n1. Visualizar busca espec√≠fica")
        print("2. Voltar")
        
        opcao = input(f"\n{Fore.YELLOW}Escolha uma op√ß√£o: {Style.RESET_ALL}")
        
        if opcao == '1':
            num = int(input(f"{Fore.YELLOW}N√∫mero da busca: {Style.RESET_ALL}")) - 1
            if 0 <= num < len(arquivos):
                carregar_resultados(os.path.join(PASTA_RESULTADOS, arquivos[::-1][num]))
            else:
                print(f"{Fore.RED}N√∫mero inv√°lido!{Style.RESET_ALL}")
                time.sleep(1)
    except Exception as e:
        print(f"\n{Fore.RED}Erro ao acessar hist√≥rico: {str(e)}{Style.RESET_ALL}")
        time.sleep(2)

def carregar_resultados(caminho_arquivo):
    try:
        with open(caminho_arquivo, 'r', encoding='utf-8') as f:
            dados = json.load(f)
        
        mostrar_resultados_eriknet(dados['username'], dados['results'])
        input(f"\n{Fore.YELLOW}Pressione Enter para continuar...{Style.RESET_ALL}")
    except Exception as e:
        print(f"\n{Fore.RED}Erro ao carregar resultados: {str(e)}{Style.RESET_ALL}")
        time.sleep(2)

def executar_busca():
    while True:
        opcao = menu_principal()
        
        if opcao == 1:
            username = input(f"\n{Fore.YELLOW}Digite o nome de usu√°rio: {Style.RESET_ALL}").strip()
            
            if not validar_username(username):
                print(f"{Fore.RED}‚ùå Nome de usu√°rio inv√°lido! Deve ter entre 3-30 caracteres (letras, n√∫meros, ., _){Style.RESET_ALL}")
                time.sleep(2)
                continue
                
            print(f"\n{Fore.YELLOW}‚è≥ Buscando por '@{username}' em 30 redes sociais (usando {MAX_THREADS} threads)...{Style.RESET_ALL}")
            
            try:
                resultados = buscar_perfis_paralelo(username)
                mostrar_resultados_eriknet(username, resultados)
                
                # Salva os resultados
                arquivo = salvar_resultados(username, resultados)
                if arquivo:
                    print(f"\n{Fore.GREEN}‚úÖ Resultados salvos em: {arquivo}{Style.RESET_ALL}")
            except Exception as e:
                print(f"\n{Fore.RED}‚ùå Erro durante a busca: {str(e)}{Style.RESET_ALL}")
                
        elif opcao == 2:
            ver_historico()
            continue
            
        elif opcao == 3:
            print(f"\n{Fore.GREEN}Saindo do ErikNet...{Style.RESET_ALL}")
            break
            
        else:
            print(f"\n{Fore.RED}‚ùå Op√ß√£o inv√°lida! Tente novamente.{Style.RESET_ALL}")
            time.sleep(1)
            
        input(f"\n{Fore.YELLOW}Pressione Enter para continuar...{Style.RESET_ALL}")

if __name__ == "__main__":
    try:
        # Verifica depend√™ncias
        try:
            import pyperclip
        except ImportError:
            print(f"{Fore.YELLOW}Aviso: pyperclip n√£o instalado. Recursos de copiar URL n√£o estar√£o dispon√≠veis.{Style.RESET_ALL}")
            print(f"{Fore.YELLOW}Instale com: pip install pyperclip{Style.RESET_ALL}")
            time.sleep(2)
        
        executar_busca()
    except KeyboardInterrupt:
        print(f"\n\n{Fore.RED}ErikNet interrompido pelo usu√°rio!{Style.RESET_ALL}")
    except Exception as e:
        print(f"\n{Fore.RED}ERRO CR√çTICO: {str(e)}{Style.RESET_ALL}")
    finally:
        print(f"\n{Fore.GREEN}Obrigado por usar o ErikNet! Seguran√ßa sempre.{Style.RESET_ALL}\n")
